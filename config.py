import os
import warnings

# ðŸ”¹ UyarÄ±larÄ± kapatma
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", message="FP16 is not supported on CPU; using FP32 instead")

# ðŸ”¹ Model klasÃ¶rÃ¼ ayarÄ±
MODEL_FOLDER = os.path.join(os.path.expanduser("~"), "Desktop", "WhisperPy", "Models")
os.makedirs(MODEL_FOLDER, exist_ok=True)  # EÄŸer klasÃ¶r yoksa oluÅŸtur

# ðŸ”¹ KullanÄ±labilir modeller listesi
MODEL_LIST = [
    "tiny", "tiny.en", "base", "base.en",
    "small", "small.en", "medium", "medium.en",
    "large", "large-v2", "large-v3", "whisper-turbo"
]

# ðŸ”¹ Model gereksinimleri
MODEL_REQUIREMENTS = {
    "tiny":       {"ram": "2GB+",  "notes": "Fast, lower accuracy"},
    "tiny.en":    {"ram": "2GB+",  "notes": "English-only version"},
    "base":       {"ram": "4GB+",  "notes": "Base model"},
    "base.en":    {"ram": "4GB+",  "notes": "English-only version"},
    "small":      {"ram": "5GB+",  "notes": "Smaller, higher accuracy"},
    "small.en":   {"ram": "6GB+",  "notes": "English-only version"},
    "medium":     {"ram": "8GB+",  "notes": "Medium model"},
    "large":      {"ram": "12GB+", "notes": "Large model"},
    "large-v2":   {"ram": "15GB+", "notes": "Latest large model"},
    "large-v3":   {"ram": "16GB+", "notes": "Newest large model"},
    "whisper-turbo": {"ram": "8GB+", "notes": "Optimized for fast transcription"}
}

# ðŸ”¹ Eksik modÃ¼lleri kontrol et
missing_modules = set()
for module in ["whisper", "torch", "psutil", "GPUtil"]:
    try:
        __import__(module)
    except ImportError:
        missing_modules.add(module)

# ðŸ”¹ Transkripsiyon iÃ§in gerekli modÃ¼ller var mÄ±?
can_transcribe = "whisper" not in missing_modules and "torch" not in missing_modules